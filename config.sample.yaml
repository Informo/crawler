# General settings that will be applied to all crawlers.
crawler:
  # The user agent to use when discovering the website. It is advised to indicate
  # what the programm is and where it comes from via this setting.
  user_agent: "Mozilla/5.0 (compatible; Informo Crawler/1.0; +https://informo.network)"
  # The user agent to use when retrieving the robots.txt file on a website.
  robot_agent: "Informo Crawler"
  # The delay between two requests on a website. Note that, if a website uses
  # the "Crawl-delay" setting in its robots.txt file, this setting will be
  # substitued with the one defined in the robots.txt file.
  crawl_delay: 1

# Description of the websites to crawl. Each website in this configuration file
# will be discovered using a different crawler, and all crawlers will run in
# parallel.
websites:
  # An identifier for the website. It is advised to use only alpha-numerical
  # identifiers, even though everything should work fine.
  - identifier: acmenews
    # The URL the crawler should start discovering the website from.
    start_point: http://acmenews.tld/
    # The different CSS selectors matching each element of a news item on
    # the website.
    selectors:
      # The CSS selector matching the news item's title.
      title: "#main-article h1"
      # The CSS selector matching the news item's description. Optional.
      description: "#main-article #description"
      # The CSS selector matching the news item's content.
      content: "#main-article #content"
      # The CSS selector matching the news item's author. Optional.
      author: "#main-article #author"
      # The CSS selector matching the news item's date.
      date: "#main-article time"
      # The CSS selector matching the news item's thumbnail. If provided, and a
      # thumbnail is found in an item, it will be prepended to the item's content.
      # It should always refer to an <img.../> node. Optional.
      thumbnail: "#main-article img.thumbnail"
    # The format of the date as it is displayed on the website. It is used for
    # parsing the news items' dates. It contains patterns, an explicit list of
    # which is included in the project's README.md file.
    date_format: "{MONTH_NUM}-{DAY_NUM}-{YEAR_LONG}"
    # Maximum number of requests sent to a website in a single run of the crawler,
    # without taking into account the request made to fetch the robots.txt file.
    # If not provided, or set to 0, doesn't limit the number of requests. Optional.
    max_visits: 200
    # How to handle the query part (i.e. the "?foo=bar&baz=qux" part) of the URL.
    # Optional.
    query:
      # If set to true, ignore the query part of every URL. This means that, if
      # http://example.tld/news?item=42 has already been visited (or queued for
      # visiting) by the crawler, it will ignore both http://example.tld/news?item=314
      # and http://example.tld/news because it will see all of them as the same URL.
      # If set to false, it will do the same as if the "query" isn't present,
      # but can take exceptions (see below).
      # Required if the "query" parent is present.
      ignore_all: true
      # Exceptions to the "ignore_all" parameter. Optional.
      except:
        news
        item
    # Regular expressions to restrict the amount of pages the crawler will visit
    # and therefore speed up the whole process. Optional.
    filters:
      # The "restrict" filter filters out every URL that doesn't match the given
      # regular expression. Optional.
      restrict: "^http://acmenews.tld/news"
      # The "exclude" filer filters out everty URL that matches the given regular
      # expression. Optional.
      exclude: "^http://acmenews.tld/not-news"

# Connection settings to the database. Currently both SQLite and PostgreSQL are
# supported.
database:
  # Name of the database driver. Current supported value are "postgres" and
  # "sqlite3".
  driver: "postgres"
  # String containing the informations required for the driver to connect to the
  # database. If the driver is "postgres", see https://godoc.org/github.com/lib/pq
  # for more information on how it is constructed. If the driver is "sqlite3",
  # the string is the path to the database file (it will be created if it doesn't
  # exist).
  connection_data: "postgres://informocrawler:itsasecret@localhost/informocrawler?sslmode=disable"

# Configuration for the feeds generator. Only required for using the feeds generator,
# else it is optional.
feeds:
  # The feed type. Supported types are "rss" and "atom".
  type: rss
  # The maximum number of news item you want to display in a website's feed. Items
  # will be ordered in counter-chronological order. If there are less items in the
  # database than the number defined here, all items in the database (related to
  # the website) will be added to the website's feed.
  nb_items: 20
  # The interface the feeds will be served at.
  interface: 127.0.0.1
  # The port the feeds will be served on.
  port: 8888
